{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Tokenizers for Armenia Language: Training Process</h1>\n",
    "<h2 style=\"text-align: center;\" >Authors: Naira Maria Barseghyan and Anna Shaljyan</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing neccessary libraries\n",
    "import pandas as pd\n",
    "from BPE_tokenizer import BpeTokenizer\n",
    "from WordPiece_tokenizer import WordPieceTokenizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the Path according to your sytem and file location\n",
    "\n",
    "df = pd.read_json('/Users/nairabarseghyan/Desktop/SPRING2024/GenerativeAI/Project/Data/wiki_arm_newest.json', orient ='columns', compression = 'infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating corpus by joining all the texts under \"text\" column of the dataset\n",
    "\n",
    "corpus = ' '.join(df['text'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Byte-Pair Encoding Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the BPE Tokenizer loaded from BPE_tokenizer.py\n",
    "\n",
    "tokenizer = BpeTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2329/2329 [5:41:35<00:00,  8.80s/it]  \n"
     ]
    }
   ],
   "source": [
    "#Learning the BPE vocabulary \n",
    "\n",
    "tokenizer._learn_bpe_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the path where we want our learned BPE vocabulary to be saved\n",
    "\n",
    "tokenizer_path = Path('./armenian_bpe_tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [5, 6767, 7359, 7, 27975, 6731, 7, 8025, 6757, 12, 7, 8661, 6714]\n",
      "Encoded Tokens: ['<maj>', 'հայ', 'երեն', ' ', 'լեզվ', 'ով', ' ', 'հոդ', 'ված', 'ի', ' ', 'օրին', 'ակ']\n",
      "Decoded Text: Հայերեն լեզվով հոդվածի օրինակ\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Հայերեն լեզվով հոդվածի օրինակ\"\n",
    "\n",
    "# Encode the sample text\n",
    "encoded_ids, encoded_tokens = tokenizer.encode_text(sample_text)\n",
    "print(f\"Encoded IDs: {encoded_ids}\")\n",
    "print(f\"Encoded Tokens: {encoded_tokens}\")\n",
    "\n",
    "# Decode the encoded IDs back to text\n",
    "decoded_text = tokenizer.decode(encoded_ids)\n",
    "print(f\"Decoded Text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the WordPiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the WordPiece Tokenizer loaded from WordPiece_tokenizer.py\n",
    "\n",
    "wp_tokenizer = WordPieceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning WordPiece vocab:  20%|█▉        | 4645/23291 [1:31:47<5:16:27,  1.02s/it]"
     ]
    }
   ],
   "source": [
    "#Learning the WordPiece vocabulary \n",
    "\n",
    "wp_tokenizer._learn_wordpiece_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the path where we want our learned WordPiece vocabulary to be saved\n",
    "\n",
    "wp_tokenizer_path = Path('./armenian_wordpiece_tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_tokenizer.save(wp_tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [0, 14037, 218, 21335, 13, 3862]\n",
      "Encoded Tokens: ['<unk>', 'լեզվ', '##ով', 'հոդված', '##ի', 'օրինակ']\n",
      "Decoded Text: <unk> լեզվ ով հոդված ի օրինակ\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Հայերեն լեզվով հոդվածի օրինակ\"\n",
    "\n",
    "# Encode the sample text\n",
    "encoded_ids, encoded_tokens = wp_tokenizer.encode_text(sample_text)\n",
    "print(f\"Encoded IDs: {encoded_ids}\")\n",
    "print(f\"Encoded Tokens: {encoded_tokens}\")\n",
    "\n",
    "# Decode the encoded IDs back to text\n",
    "decoded_text = wp_tokenizer.decode(encoded_ids)\n",
    "print(f\"Decoded Text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of the training process\n",
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
